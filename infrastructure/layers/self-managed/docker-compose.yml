# If this will not give me a job as junior devops
# I do not have clue what will...
# https://docs.docker.com/compose/compose-file/05-services/#volumes
# ----------------------------------------
# Resource Presets
# ----------------------------------------
x-resource-preset-x0.25: &resource-preset-x025
  limits:
    cpus: '0.25'
    memory: 256M
x-resource-preset-x0.5: &resource-preset-x05
  limits:
    cpus: '0.5'
    memory: 512M
x-resource-preset-x1: &resource-preset-x1
  limits:
    cpus: '1'
    memory: 1G
x-resource-preset-x2: &resource-preset-x2
  limits:
    cpus: '2'
    memory: 2G
x-resource-preset-x16: &resource-preset-x16
  limits:
    cpus: '4'
    memory: 4G

# ----------------------------------------
# Logging Configuration
# ----------------------------------------
x-logging-config: &logging-default
  logging:
    driver: "json-file"
    options:
      max-size: "10m"
      max-file: "3"

# ----------------------------------------
# Restart Policy Configuration
# ----------------------------------------
x-restart-policy: &restart_policy
  condition: on-failure
  max_attempts: 3

# ----------------------------------------
# Healthcheck Configuration
# ----------------------------------------
x-healthcheck-default: &healthcheck-default
  # Avoid setting the interval too small, as docker uses much more CPU than one would expect.
  # Related issues:
  # https://github.com/moby/moby/issues/39102
  # https://github.com/moby/moby/issues/39388
  # https://github.com/getsentry/self-hosted/issues/1000
  interval: 60s
  timeout: 60s
  retries: 2
  start_period: 30s

name: "polylab"
services:
  # ----------------------------------------
  # Cloudflare Tunnel
  # ----------------------------------------
  # This is the core service that connects the
  # local development environment to the Cloudflare
  # network. It is used to expose the local development
  # environment to the internet.
  # ----------------------------------------
  tunnel:
    container_name: ${PROJECT}-tunnel
    image: cloudflare/cloudflared:latest
    restart: unless-stopped
    command: tunnel --no-autoupdate run --token ${CLOUDFLARE_TUNNEL_SERIVCE_TOKEN}
    networks:
      - infisical
      - twentycrm
      - tunnel
    deploy:
      resources: *resource-preset-x025
      restart_policy: *restart_policy
  # ----------------------------------------

  dockerproxy:
    image: ghcr.io/tecnativa/docker-socket-proxy:latest
    container_name: dockerproxy
    environment:
      - CONTAINERS=1
      - SERVICES=1
      - TASKS=1
      - POST=0
    ports:
      - 127.0.0.1:2375:2375
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro # Mounted as read-only
    restart: unless-stopped
    networks:
      - supabase
      - docker_proxy

  # ----------------------------------------
  # Server Dashboard
  # ----------------------------------------
  homepage:
      image: ghcr.io/gethomepage/homepage:latest
      container_name: homepage
      ports:
        - 3010:3000
      volumes:
        - ./homepage:/app/config
      networks:
        - docker_proxy
  # ----------------------------------------

  # ----------------------------------------
  # Object Storage
  # ----------------------------------------
  # There are multiple object storage services
  # the one I know is minio, however, since they
  # 've changed pricing and license, I'd
  # look forward to exploring something else
  # that can be used on closed-source apps.
  # ----------------------------------------
  minio:
    image: minio/minio
    container_name: ${PROJECT}-minio
    ports:
      - '${MINIO_PORT}:9000'
      - '9001:9001'
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server --console-address ":9001" /data
    volumes:
      - minio_data:/data:z
    deploy:
      resources: *resource-preset-x1
      restart_policy: *restart_policy
    networks:
      - supabase

  minio-migration:
    image: minio/mc
    container_name: ${PROJECT}-minio-migration
    env_file:
      - .env
    #    depends_on:
    #      minio:
    #        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set minio http://${MINIO_HOST}:${MINIO_PORT} ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      /usr/bin/mc mb minio/stub;
      /usr/bin/mc mb minio/dummy;
      exit 0;
      "
    deploy:
      resources: *resource-preset-x025
      restart_policy: *restart_policy
    networks:
      - supabase

  # ----------------------------------------
  # Object Management
  # ----------------------------------------
  imgproxy:
    container_name: ${PROJECT}-imgproxy
    image: darthsim/imgproxy:v3.8.0
    healthcheck:
      <<: *healthcheck-default
      test: [ "CMD", "imgproxy", "health" ]
    environment:
      IMGPROXY_BIND: ":5001"
      IMGPROXY_USE_ETAG: "true"
      IMGPROXY_ENABLE_WEBP_DETECTION: true
    deploy:
      resources: *resource-preset-x025
      restart_policy: *restart_policy
    networks:
      - supabase

  # ----------------------------------------
  # Cache
  # ----------------------------------------
  redis:
    image: redis
    container_name: redis
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
    networks:
      - infisical
      - cache
    volumes:
      - redis_data:/data
    healthcheck:
      <<: *healthcheck-default
      test: redis-cli ping
    deploy:
      resources: *resource-preset-x05
      restart_policy: *restart_policy
  # TODO: Memcached
  # TODO: Dragonfly

  # ----------------------------------------
  # Database
  # ----------------------------------------
  postgres:
    container_name: postgres
    image: supabase/postgres:15.1.0.147
    healthcheck:
      <<: *healthcheck-default
      test: pg_isready -U postgres -h localhost
    depends_on:
      supabase-vector:
        condition: service_started
    command:
      - postgres
      - -c
      - config_file=/etc/postgresql/postgresql.conf
      - -c
      - log_min_messages=fatal
    ports:
      - ${DOCKER_POSTGRES_EXTERNAL_PORT}:${POSTGRES_PORT}
    environment:
      POSTGRES_HOST: /var/run/postgresql
      PGPORT: ${POSTGRES_PORT}
      POSTGRES_PORT: ${POSTGRES_PORT}
      PGPASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATABASE: ${POSTGRES_DB}
      POSTGRES_DB: ${POSTGRES_DB}
      JWT_SECRET: ${SUPABASE_JWT_SECRET}
      JWT_EXP: ${SUPABASE_JWT_EXPIRY}
    volumes:
      - ./supabase/db/realtime.sql:/docker-entrypoint-initdb.d/migrations/99-realtime.sql:Z
      # Must be superuser to create event trigger
      - ./supabase/db/webhooks.sql:/docker-entrypoint-initdb.d/init-scripts/98-webhooks.sql:Z
      # Must be superuser to alter reserved role
      - ./supabase/db/roles.sql:/docker-entrypoint-initdb.d/init-scripts/99-roles.sql:Z
      # Initialize the database settings with JWT_SECRET and JWT_EXP
      - ./supabase/db/jwt.sql:/docker-entrypoint-initdb.d/init-scripts/99-jwt.sql:Z
      # PGDATA directory is persisted between restarts
      - postgres_data:/var/lib/postgresql/data:Z
      # Changes required for Analytics support
      - ./supabase/db/logs.sql:/docker-entrypoint-initdb.d/migrations/99-logs.sql:Z
      # Use named volume to persist pgsodium decryption key between restarts
      - pg_config:/etc/postgresql-custom
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init-scripts/00-plygrnd.sql
    deploy:
      resources: *resource-preset-x1
      restart_policy: *restart_policy
    networks:
      - supabase
      - postgres
      - infisical
      - twentycrm

  cockroachdb:
    # TODO: Prepare multi-node setup with certificates and secure communication.
    image: cockroachdb/cockroach:v23.2.2
    container_name: cockroach
    ports:
      - "${COCKROACHDB_PORT}:${COCKROACHDB_PORT}"
      - "${COCKROACHDB_CONSOLE_PORT}:8080"
    command: start-single-node --insecure
    volumes:
      - "cockroach_data:/cockroach/cockroach-data"
#      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    deploy:
      # The required resources for this service are min. x4
      resources: *resource-preset-x1
      restart_policy: *restart_policy

  # ----------------------------------------
  # Messaging
  # ----------------------------------------

  # ----------------------------------------
  # Core Services
  # ----------------------------------------
  # These are the core services that are required
  # to run the application.
  # ----------------------------------------
  # TODO: Kafka
  # TODO: Redis
  # TODO: Memcached
  # TODO: RabbitMQ
  # TODO: Elasticsearch
  # TODO: Kibana
  # TODO: Melisearch
  # TODO: Minio
  # TODO: IPFS
  # ----------------------------------------

  # ----------------------------------------
  # Observability Services
  # ----------------------------------------
  # These are the core services that are required
  # to run the application.
  # ----------------------------------------
  prometheus:
    image: prom/prometheus
    restart: always
    volumes:
      - ./prometheus:/etc/prometheus/
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    ports:
      - 9091:9090
    depends_on:
      - node-exporter
      - cadvisor
    deploy:
      resources: *resource-preset-x025
      restart_policy: *restart_policy
    networks:
      - monitoring

  node-exporter:
    image: prom/node-exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - --collector.filesystem.ignored-mount-points
      - '^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)'
    ports:
      - 9100:9100
    restart: always
    deploy:
      mode: global
      resources: *resource-preset-x025
      restart_policy: *restart_policy
    networks:
      - monitoring

  cadvisor:
      image: gcr.io/cadvisor/cadvisor
      container_name: cadvisor
      privileged: true
      cgroup: host
      userns_mode: "host"
      ipc: private
      shm_size: 128M
      expose:
        - "8080"
      networks:
        - monitoring
      volumes:
        - /:/rootfs:ro
        - /var/run:/var/run:rw
        - /sys:/sys:ro
        - /var/lib/docker/:/var/lib/docker:ro
        - /dev/disk/:/dev/disk:ro
        # Fucking cgroup bullshit
        - /sys/fs/cgroup:/sys/fs/cgroup:ro
      deploy:
        mode: global
        resources: *resource-preset-x025
        restart_policy: *restart_policy

  grafana:
    image: grafana/grafana
    user: '472'
    restart: always
    environment:
      - GF_INSTALL_PLUGINS=grafana-clock-panel
      - GF_RENDERING_SERVER_URL=http://renderer:8081/render
      - GF_RENDERING_CALLBACK_URL=http://grafana:3000/
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning/:/etc/grafana/provisioning/
    env_file:
      - ./grafana/config.monitoring
    ports:
      - 3000:3000
    depends_on:
      - prometheus
    deploy:
      resources: *resource-preset-x025
      restart_policy: *restart_policy
    networks:
      - monitoring

  renderer:
    image: grafana/grafana-image-renderer:latest
    ports:
      - 8081
    deploy:
      resources: *resource-preset-x025
      restart_policy: *restart_policy
    networks:
      - monitoring

  # TODO: HyperDX
  # TODO: Grafana
  # TODO: Prometheus
  # TODO: Jaeger
  # TODO: Zipkin

  # ----------------------------------------
  # Infisical
  # ----------------------------------------
  infisical:
    container_name: ${PROJECT}-infisical
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      infisical-migration:
        condition: service_completed_successfully
    image: infisical/infisical:latest-postgres
    ports:
      - ${INFISICAL_PORT}:8080
    environment: &infisical-env
      - NODE_ENV=production
      - SITE_URL=${INFISICAL_SITE_URL}
      - DB_CONNECTION_URI=${INFISICAL_DB_CONNECTION_URI}
      - AUTH_SECRET=${INFISICAL_AUTH_SECRET}
      - ENCRYPTION_KEY=${INFISICAL_ENCRYPTION_KEY}
      - REDIS_URL=${INFISICAL_REDIS_URL}
      - SMTP_HOST=${SMTP_HOST}
      - SMTP_PORT=${SMTP_PORT}
      - SMTP_NAME=${SMTP_NAME}
      - SMTP_USERNAME=${SMTP_USERNAME}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
    networks:
      - infisical
      - cache
    deploy:
      resources: *resource-preset-x1
  infisical-migration:
    container_name: infisical-migration
    depends_on:
      postgres:
        condition: service_healthy
    image: infisical/infisical:latest-postgres
    command: npm run migration:latest
    environment: *infisical-env
    networks:
      - infisical
    deploy:
      resources: *resource-preset-x1
  # ----------------------------------------

  # ----------------------------------------
  # Supabase
  # ----------------------------------------
  # TODO: Add Supabase service.
  # ----------------------------------------

  # ----------------------------------------
  # Sentry
  # ----------------------------------------
  # TODO: Add Sentry service.
  # ----------------------------------------

  # ----------------------------------------
  # TwentyCRM
  # ----------------------------------------

  twenty:
    image: twentycrm/twenty-front:latest
    container_name: ${PROJECT}-twenty
    ports:
      - ${TWENTY_PORT}:3000
    environment:
      - SIGN_IN_PREFILLED=${TWENTY_SIGN_IN_PREFILLED}
      - REACT_APP_SERVER_BASE_URL=${TWENTY_SERVER_URL}
      - REACT_APP_SERVER_AUTH_URL=${TWENTY_SERVER_URL}/auth
      - REACT_APP_SERVER_FILES_URL=${TWENTY_SERVER_URL}/files
    depends_on:
      - twenty-server
    networks:
      - twentycrm
    deploy:
      resources: *resource-preset-x025
  twenty-server:
    image: twentycrm/twenty-server:latest
    container_name: twenty-server
    ports:
      - ${TWENTY_SERVER_PORT}:${TWENTY_SERVER_PORT}
    environment: &twenty-server-env
      - SIGN_IN_PREFILLED=${TWENTY_SIGN_IN_PREFILLED}
      - PG_DATABASE_URL=${TWENTY_DATABASE_URL}
      - FRONT_BASE_URL=${TWENTY_SITE_URL}
      - PORT=${TWENTY_SERVER_PORT}
      - STORAGE_TYPE=local
      - STORAGE_LOCAL_PATH=.local-storage
      - ACCESS_TOKEN_SECRET=${TWENTY_ACCESS_TOKEN_SECRET}
      - LOGIN_TOKEN_SECRET=${TWENTY_LOGIN_TOKEN_SECRET}
      - REFRESH_TOKEN_SECRET=${TWENTY_REFRESH_TOKEN_SECRET}
    depends_on:
      - twenty-database
    networks:
      - twentycrm
    deploy:
      resources: *resource-preset-x2
  twenty-migrate:
    image: twentycrm/twenty-server:latest
    container_name: twenty-migrate
    command: yarn nx database:init
    environment: *twenty-server-env
    depends_on:
      - twenty-database
    networks:
      - twentycrm
    deploy:
      resources: *resource-preset-x2
  twenty-database:
    container_name: ${PROJECT}-twenty-database
    image: twentycrm/twenty-postgres:latest
    volumes:
      - twenty_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=twenty
    networks:
      - twentycrm
    deploy:
      resources: *resource-preset-x025

  # ----------------------------------------
  # Bytebase
  # ----------------------------------------
  bytebase:
    image: bytebase/bytebase:2.14.0
    container_name: bytebase
    ports:
      - ${BYTEBASE_PORT}:8080
    volumes:
      - bytebase_data:/var/opt/bytebase
    networks:
      - postgres
      - tunnel
    deploy:
      resources: *resource-preset-x1
      restart_policy: *restart_policy

  # ----------------------------------------
  # mathesar
  # ----------------------------------------

  mathesar:
    image: mathesar/mathesar-prod:latest
    container_name: mathesar
    ports:
      - ${MATHESAR_PORT}:8000
    networks:
      - postgres
      - tunnel
    deploy:
        resources: *resource-preset-x1
        restart_policy: *restart_policy

    # ----------------------------------------
    # Blockchain
    # ----------------------------------------
    # These are the core services that are required
    # to run the application.
    # ----------------------------------------

    # ----------------------------------------
    # Bitcoin Node
    # ----------------------------------------
#  bitcoind:
#      container_name: bitcoind
#      image: lncm/bitcoind:v22.0@sha256:37a1adb29b3abc9f972f0d981f45e41e5fca2e22816a023faa9fdc0084aa4507
#      volumes:
#        - bitcoin_data:/data/.bitcoin
#      restart: on-failure
#      stop_grace_period: 15m30s
#      ports:
#        - "18443:18443"
#        # - "8333:8333" # "$BITCOIN_P2P_PORT:$BITCOIN_P2P_PORT"
##  bitcoind_dashboard:
##    image: umbrel-bitcoin:master
##    container_name: bitcoind-dashboard
##    depends_on: [bitcoind]
##    command: ["npm", "start"]
##    restart: on-failure
##    ports:
##      - "10075:3005"
##    environment:
##      PORT: "3005"
##      BITCOIN_HOST: "bitcoind"
##      RPC_PORT: $BITCOIN_RPC_PORT
##      RPC_USER: $BITCOIN_RPC_USER
##      RPC_PASSWORD: $BITCOIN_RPC_PASS
##      BITCOIN_RPC_HIDDEN_SERVICE: "/var/lib/tor/bitcoin-rpc/hostname"
##      BITCOIN_P2P_HIDDEN_SERVICE: "/var/lib/tor/bitcoin-p2p/hostname"
#
#    # ----------------------------------------
#    # Backups and Volume Snapshotting
#    # ----------------------------------------
#    # docker-compose and volume management is
#    # kinda pain in the ass comparing it to local
#    # aka block volume management, I'm sure there
#    # are tools that avoid such struggle with volumes
#    # such as automatic backup to some S3 and easy
#    # recovery.
#    # ----------------------------------------

# TODO: Check Docker Volume Plugins as they may be useful for managing these volumes, also after quick
#   research I think they would have me in my previous production deployments based on docker-compose.include:
#   https://github.com/rexray/rexray
#   https://github.com/rancher/convoy
#   https://github.com/MatchbookLab/local-persist
#   https://github.com/ScatterHQ/flocker
# TODO(https://github.com/keinsell/plygrnd/issues/335): ZFS and Docker Volumes?
volumes:
  redis_data:
    name: redis
    driver: local
  minio_data:
    name: minio
    driver: local
  twenty_data:
    name: twenty
    driver: local
  cockroach_data:
    name: cockroach
    driver: local
  prometheus_data:
    name: prometheus
    driver: local
  grafana_data:
    name: grafana
    driver: local
  bytebase_data:
    name: bytebase
    driver: local
  pg_config:
    name: pgconfig
    driver: local
  postgres_data:
    name: postgres_data
    driver: local
  supabase_storage_data:
    name: supabase-storage
    driver: local
  bitcoin_data:
    name: bitcoin
    driver: local
networks:
  infisical:
    name: infisical
  cache:
    name: cache
  sourcegraph:
    name: sourcegraph
  supabase:
    name: supabase
  twentycrm:
    name: twentycrm
  cockroach:
    name: cockroach
  monitoring:
    name: monitoring
    driver: bridge
  postgres:
    name: postgres
    driver: bridge
  tunnel:
    name: tunnel
  bitcoin:
    name: bitcoin
  docker_proxy:
    name: docker_proxy

# TODO: Learn how to utilize secrets.
secrets:
  postgres_password:
    file: ./.env
    name: postgres_password

# TODO: Learn how to utilize configs.
configs:
  postgres_config:
    name: postgres-config
    file: .env

