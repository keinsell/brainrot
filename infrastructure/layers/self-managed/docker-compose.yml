# If this will not give me a job as junior devops
# I do not have clue what will...
# https://docs.docker.com/compose/compose-file/05-services/#volumes
# ----------------------------------------
# Resource Presets
# ----------------------------------------
x-resource-preset-x0.25: &resource-preset-x025
  limits:
    cpus: '0.25'
    memory: 256M
x-resource-preset-x0.5: &resource-preset-x05
  limits:
    cpus: '0.5'
    memory: 512M
x-resource-preset-x1: &resource-preset-x1
  limits:
    cpus: '1'
    memory: 1G
x-resource-preset-x2: &resource-preset-x2
  limits:
    cpus: '2'
    memory: 2G
x-resource-preset-x16: &resource-preset-x16
  limits:
    cpus: '4'
    memory: 4G

# ----------------------------------------
# Logging Configuration
# ----------------------------------------
x-logging-config: &logging-default
  logging:
    driver: "json-file"
    options:
      max-size: "10m"
      max-file: "3"

# ----------------------------------------
# Restart Policy Configuration
# ----------------------------------------
x-restart-policy: &restart_policy
  condition: on-failure
  max_attempts: 3

# ----------------------------------------
# Stateful Persistence Volumes Configuration
# ----------------------------------------
# My problem there is that I know bind mounts would solve my
# problem, but there I would need to define tons of bind mounts
# into different places, and I would like to seek for more elegant
# way that would allow me to define the volumes in one place and
# then use them in different services.
# I'm actually sure the volume plugins are things that I need
# but before them I need one bind mount for ex.
# S3 bucket that would store all the data that I need to persist
#
# https://rclone.org/docker/
# docker plugin set rclone RCLONE_VERBOSE=2 config=/etc/rclone args="--vfs-cache-mode=writes --allow-other
# Local Persist ("local-persist")
# https://github.com/MatchbookLab/local-persist/tree/master
# https://github.com/rexray/rexray
# https://github.com/offen/docker-volume-backup
# This project used NFS volumes for persisting data
# https://github.com/tldr-devops/startpack/tree/main
# https://github.com/ethibox/awesome-stacks/tree/master
# https://github.com/juicedata/docker-volume-juicefs
# https://github.com/TrilliumIT/docker-zfs-plugin
# glusterfs plugin
# ---
# docker run -d \
#   -v /run/docker/plugins/:/run/docker/plugins/ \
#   -v /spacewarp/docker/plugins/local-persist:/var/lib/docker/plugin-data/ \
#   -v /spacewarp/docker/volumes:/data \
#      cwspear/docker-local-persist-volume-plugin
#
x-stateful-volume-default: &stateful-volume
  driver: local

# ----------------------------------------
# Healthcheck Configuration
# ----------------------------------------
x-healthcheck-default:
  # Avoid setting the interval too small, as docker uses much more CPU than one would expect.
  # Related issues:
  # https://github.com/moby/moby/issues/39102
  # https://github.com/moby/moby/issues/39388
  # https://github.com/getsentry/self-hosted/issues/1000
  &healthcheck-default
  interval: 60s
  timeout: 60s
  retries: 2
  start_period: 30s

name: "polylab"
services:
  dockerproxy:
    image: ghcr.io/tecnativa/docker-socket-proxy:latest
    container_name: dockerproxy
    environment:
      - CONTAINERS=1
      - SERVICES=1
      - TASKS=1
      - POST=0
    ports:
      - 127.0.0.1:2375:2375
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro # Mounted as read-only
    restart: unless-stopped
    networks:
      - supabase
      - docker_proxy

  # ----------------------------------------
  # Server Dashboard
  # ----------------------------------------
  # There is a need for two dashboards, the one
  # for the public network and people who ten use
  # my infrastructure and applications and
  # the second one for the internal network and
  # mostly myself.
  #
  # https://github.com/ProductiveOps/dokemon
  # https://github.com/louislam/dockge?tab=readme-ov-file
  # https://github.com/hywax/mafl
  # https://github.com/ordinary-dev/phoenix
  # ----------------------------------------
  homepage:
    image: ghcr.io/gethomepage/homepage:latest
    container_name: homepage
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.homepage.rule=PathPrefix(`/`)"
      - "traefik.http.routers.homepage.entrypoints=http"
    ports:
      - 3010:3000
    volumes:
      - type: bind
        source: ./homepage
        target: /app/config
    networks:
      - docker_proxy
      - reverse_proxy
  # ----------------------------------------

  # ----------------------------------------
  # Object Storage
  # ----------------------------------------
  # There are multiple object storage services
  # the one I know is minio, however, since they
  # 've changed pricing and license, I'd
  # look forward to exploring something else
  # that can be used on closed-source apps.
  # ----------------------------------------
  minio:
    image: minio/minio
    container_name: minio
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.minio.rule=Host(`minio.${DOMAIN}`)"
      - "traefik.http.services.minio.loadbalancer.server.port=9001"
      - "traefik.http.routers.minio.entrypoints=http"
    ports:
      - '${MINIO_PORT}:9000'
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server --console-address ":9001" /data
    volumes:
      - type: bind
        source: /srv/minio
        target: /data
    deploy:
      resources: *resource-preset-x1
      restart_policy: *restart_policy
    networks:
      - supabase
      - reverse_proxy

  minio-migration:
    image: minio/mc
    container_name: minio-migration
    env_file:
      - .env
    entrypoint: >
      /bin/sh -c " /usr/bin/mc alias set minio http://${MINIO_HOST}:${MINIO_PORT} ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}; /usr/bin/mc mb minio/stub; /usr/bin/mc mb minio/dummy; usr/bin/mc mb minio/docker; exit 0;"
    deploy:
      resources: *resource-preset-x025
      restart_policy: *restart_policy
    networks:
      - supabase

  # ----------------------------------------
  # Object Management
  # ----------------------------------------
  imgproxy:
    container_name: ${PROJECT}-imgproxy
    image: darthsim/imgproxy:v3.8.0
    healthcheck:
      <<: *healthcheck-default
      test: [ "CMD", "imgproxy", "health" ]
    environment:
      IMGPROXY_BIND: ":5001"
      IMGPROXY_USE_ETAG: "true"
      IMGPROXY_ENABLE_WEBP_DETECTION: true
    deploy:
      resources: *resource-preset-x025
      restart_policy: *restart_policy
    networks:
      - supabase

  # ----------------------------------------
  # Cache
  # ----------------------------------------
  redis:
    image: redis
    container_name: redis
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
    networks:
      - infisical
      - cache
    volumes:
      - redis_data:/data
    healthcheck:
      <<: *healthcheck-default
      test: redis-cli ping
    deploy:
      resources: *resource-preset-x05
      restart_policy: *restart_policy
  # TODO: Memcached
  # TODO: Dragonfly

  # TODO: Setup k3s
  # https://doc.traefik.io/traefik/user-guides/crd-acme/

  # ----------------------------------------
  # Database
  # ----------------------------------------
  postgres:
    container_name: postgres
    image: supabase/postgres:15.1.0.147
    healthcheck:
      <<: *healthcheck-default
      test: pg_isready -U postgres -h localhost
    command:
      - postgres
      - -c
      - config_file=/etc/postgresql/postgresql.conf
      - -c
      - log_min_messages=fatal
    ports:
      - ${POSTGRES_PORT}:${POSTGRES_PORT}
    environment:
      POSTGRES_HOST: /var/run/postgresql
      PGPORT: ${POSTGRES_PORT}
      POSTGRES_PORT: ${POSTGRES_PORT}
      PGPASSWORD: /run/secrets/postgres_password
      POSTGRES_PASSWORD: /run/secrets/postgres_password
      PGDATABASE: ${POSTGRES_DB}
      POSTGRES_DB: ${POSTGRES_DB}
      JWT_SECRET: ${SUPABASE_JWT_SECRET}
      JWT_EXP: ${SUPABASE_JWT_EXPIRY}
    volumes:
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init-scripts/00-plygrnd.sql
    deploy:
      resources: *resource-preset-x1
      restart_policy: *restart_policy
    networks:
      - supabase
      - postgres
      - infisical
      - twentycrm
    secrets:
      - source: postgres_password

  hydra:
    container_name: hydra
    image: ghcr.io/hydradatabase/hydra:latest
    ports:
      - ${HYDRA_EXTERNAL_PORT}:${HYDRA_PORT}
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - hydra_data:/var/lib/postgresql/data
      - ./hydra/postgresql.conf:/etc/postgresql/postgresql.conf
    command: postgres -c 'config_file=/etc/postgresql/postgresql.conf'
    networks:
      - postgres

  cockroachdb:
    # TODO: Prepare multi-node setup with certificates and secure communication.
    image: cockroachdb/cockroach:v23.2.2
    container_name: cockroach
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.cockroachdb.rule=Host(`cockroach.${DOMAIN}`)"
      - "traefik.http.services.cockroachdb.loadbalancer.server.port=8080"
      - "traefik.http.routers.cockroachdb.entrypoints=http"
    ports:
      - "${COCKROACHDB_PORT}:${COCKROACHDB_PORT}"
    command: start-single-node --insecure
    volumes:
      - "cockroach_data:/cockroach/cockroach-data"
    #      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    deploy:
      # The required resources for this service are min. x4
      resources: *resource-preset-x1
      restart_policy: *restart_policy
    networks:
      - reverse_proxy

  # ----------------------------------------
  # Messaging
  # ----------------------------------------

  # ----------------------------------------
  # Core Services
  # ----------------------------------------
  # These are the core services that are required
  # to run the application.
  # ----------------------------------------
  # TODO: Kafka
  # TODO: Redis
  # TODO: Memcached
  # TODO: RabbitMQ
  # TODO: Elasticsearch
  # TODO: Kibana
  # TODO: Melisearch
  # TODO: Minio
  # TODO: IPFS
  # ----------------------------------------

  # ----------------------------------------
  # Observability Services
  # ----------------------------------------
  # These are the core services that are required
  # to run the application.
  # ----------------------------------------
  prometheus:
    image: prom/prometheus
    restart: always
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.prometheus.rule=Host(`prometheus.${DOMAIN}`)"
      - "traefik.http.services.prometheus.loadbalancer.server.port=9090"
      - "traefik.http.routers.prometheus.entrypoints=http"
    volumes:
      - ./prometheus:/etc/prometheus/
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    depends_on:
      - node-exporter
      - cadvisor
    deploy:
      resources: *resource-preset-x025
      restart_policy: *restart_policy
    networks:
      - monitoring

  node-exporter:
    image: prom/node-exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - --collector.filesystem.ignored-mount-points
      - '^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)'
    ports:
      - 9100:9100
    restart: always
    deploy:
      mode: global
      resources: *resource-preset-x025
      restart_policy: *restart_policy
    networks:
      - monitoring

  # https://gvisor.dev/
  cadvisor:
    image: gcr.io/cadvisor/cadvisor
    container_name: cadvisor
    privileged: true
    cgroup: host
    userns_mode: "host"
    ipc: private
    shm_size: 128M
    expose:
      - "8080"
    networks:
      - monitoring
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
      # Fucking cgroup bullshit
      - /sys/fs/cgroup:/sys/fs/cgroup:ro
    deploy:
      mode: global
      resources: *resource-preset-x025
      restart_policy: *restart_policy

  grafana:
    image: grafana/grafana
    user: '472'
    restart: always
    environment:
      - GF_INSTALL_PLUGINS=grafana-clock-panel
      - GF_RENDERING_SERVER_URL=http://renderer:8081/render
      - GF_RENDERING_CALLBACK_URL=http://grafana:3000/
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning/:/etc/grafana/provisioning/
    env_file:
      - 11_monitoring/grafana/config.monitoring
    ports:
      - 3000:3000
    depends_on:
      - prometheus
    deploy:
      resources: *resource-preset-x025
      restart_policy: *restart_policy
    networks:
      - monitoring

  renderer:
    image: grafana/grafana-image-renderer:latest
    ports:
      - 8081
    deploy:
      resources: *resource-preset-x025
      restart_policy: *restart_policy
    networks:
      - monitoring

  # TODO: HyperDX
  # TODO: Grafana
  # TODO: Prometheus
  # TODO: Jaeger
  # TODO: Zipkin

  # ----------------------------------------
  # Infisical
  # ----------------------------------------
  infisical:
    container_name: infisical
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      infisical-migration:
        condition: service_completed_successfully
    image: infisical/infisical:latest-postgres
    ports:
      - ${INFISICAL_PORT}:8080
    environment: &infisical-env
      - NODE_ENV=production
      - SITE_URL=${INFISICAL_SITE_URL}
      - DB_CONNECTION_URI=${INFISICAL_DB_CONNECTION_URI}
      - AUTH_SECRET=${INFISICAL_AUTH_SECRET}
      - ENCRYPTION_KEY=${INFISICAL_ENCRYPTION_KEY}
      - REDIS_URL=${INFISICAL_REDIS_URL}
      - SMTP_HOST=${SMTP_HOST}
      - SMTP_PORT=${SMTP_PORT}
      - SMTP_NAME=${SMTP_NAME}
      - SMTP_USERNAME=${SMTP_USERNAME}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
    networks:
      - infisical
      - cache
    deploy:
      resources: *resource-preset-x1
  infisical-migration:
    container_name: infisical-migration
    depends_on:
      postgres:
        condition: service_healthy
    image: infisical/infisical:latest-postgres
    command: npm run migration:latest
    environment: *infisical-env
    networks:
      - infisical
    deploy:
      resources: *resource-preset-x1
  # ----------------------------------------

  # ----------------------------------------
  # Supabase
  # ----------------------------------------
  # TODO: Add Supabase service.
  # ----------------------------------------

  # ----------------------------------------
  # Sentry
  # ----------------------------------------
  # TODO: Add Sentry service.
  # ----------------------------------------

  # ----------------------------------------
  # TwentyCRM
  # ----------------------------------------

  twenty:
    image: twentycrm/twenty-front:latest
    container_name: ${PROJECT}-twenty
    ports:
      - ${TWENTY_PORT}:3000
    environment:
      - SIGN_IN_PREFILLED=${TWENTY_SIGN_IN_PREFILLED}
      - REACT_APP_SERVER_BASE_URL=${TWENTY_SERVER_URL}
      - REACT_APP_SERVER_AUTH_URL=${TWENTY_SERVER_URL}/auth
      - REACT_APP_SERVER_FILES_URL=${TWENTY_SERVER_URL}/files
    depends_on:
      - twenty-server
    networks:
      - twentycrm
    deploy:
      resources: *resource-preset-x025
  twenty-server:
    image: twentycrm/twenty-server:latest
    container_name: twenty-server
    ports:
      - ${TWENTY_SERVER_PORT}:${TWENTY_SERVER_PORT}
    environment: &twenty-server-env
      - SIGN_IN_PREFILLED=${TWENTY_SIGN_IN_PREFILLED}
      - PG_DATABASE_URL=${TWENTY_DATABASE_URL}
      - FRONT_BASE_URL=${TWENTY_SITE_URL}
      - PORT=${TWENTY_SERVER_PORT}
      - STORAGE_TYPE=local
      - STORAGE_LOCAL_PATH=.local-storage
      - ACCESS_TOKEN_SECRET=${TWENTY_ACCESS_TOKEN_SECRET}
      - LOGIN_TOKEN_SECRET=${TWENTY_LOGIN_TOKEN_SECRET}
      - REFRESH_TOKEN_SECRET=${TWENTY_REFRESH_TOKEN_SECRET}
    depends_on:
      - twenty-database
    networks:
      - twentycrm
    deploy:
      resources: *resource-preset-x2
  twenty-migrate:
    image: twentycrm/twenty-server:latest
    container_name: twenty-migrate
    command: yarn nx database:init
    environment: *twenty-server-env
    depends_on:
      - twenty-database
    networks:
      - twentycrm
    deploy:
      resources: *resource-preset-x2
  # TODO: Ditch this container for postgres
  twenty-database:
    container_name: twenty-database
    image: twentycrm/twenty-postgres:latest
    volumes:
      - twenty_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=twenty
    networks:
      - twentycrm
    deploy:
      resources: *resource-preset-x025

  # ----------------------------------------
  # Bytebase
  # ----------------------------------------
  bytebase:
    image: bytebase/bytebase:2.14.0
    container_name: bytebase
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.bytebase.rule=Host(`bytebase.${DOMAIN}`)"
      - "traefik.http.services.bytebase.loadbalancer.server.port=8080"
      - "traefik.http.routers.bytebase.entrypoints=http"
    ports:
      - 10034:8080
    volumes:
      - ~/srv/bytebase:/var/opt/bytebase
    networks:
      - reverse_proxy
    deploy:
      resources: *resource-preset-x1
      restart_policy: *restart_policy

  # ----------------------------------------
  # mathesar
  # ----------------------------------------

  mathesar:
    image: mathesar/mathesar-prod:latest
    container_name: mathesar
    ports:
      - ${MATHESAR_PORT}:8000
    networks:
      - postgres
      - tunnel
    deploy:
      resources: *resource-preset-x1
      restart_policy: *restart_policy
    depends_on:
      postgres:
        condition: service_healthy
    # tubearchivist:
    #   container_name: tubearchivist
    #   image: bbilly1/tubearchivist
    #   ports:
    #     - 8000:8000
    #   volumes:
    #     - media:/youtube
    #     - cache:/cache
    #   environment:
    #     - ES_URL=http://archivist-es:9200 # needs protocol e.g. http and port
    #     - REDIS_HOST=archivist-redis # don't add protocol
    #     - HOST_UID=1000
    #     - HOST_GID=1000
    #     - TA_HOST=tubearchivist.local # set your host name
    #     - TA_USERNAME=tubearchivist # your initial TA credentials
    #     - TA_PASSWORD=verysecret # your initial TA credentials
    #     - ELASTIC_PASSWORD=verysecret # set password for Elasticsearch
    #     - TZ=America/New_York # set your time zone
    #   healthcheck:
    #     test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
    #     interval: 2m
    #     timeout: 10s
    #     retries: 3
    #     start_period: 30s
    #   depends_on:
    #     - redis
    #     - archivist-redis
    # ----------------------------------------
    # Blockchain
    # ----------------------------------------
    # These are the core services that are required
    # to run the application.
    # ----------------------------------------

    # ----------------------------------------
    # Bitcoin Node
    # ----------------------------------------
    #  bitcoind:
    #      container_name: bitcoind
    #      image: lncm/bitcoind:v22.0@sha256:37a1adb29b3abc9f972f0d981f45e41e5fca2e22816a023faa9fdc0084aa4507
    #      volumes:
    #        - bitcoin_data:/data/.bitcoin
    #      restart: on-failure
    #      stop_grace_period: 15m30s
    #      ports:
    #        - "18443:18443"
    #        # - "8333:8333" # "$BITCOIN_P2P_PORT:$BITCOIN_P2P_PORT"
    ##  bitcoind_dashboard:
    ##    image: umbrel-bitcoin:master
    ##    container_name: bitcoind-dashboard
    ##    depends_on: [bitcoind]
    ##    command: ["npm", "start"]
    ##    restart: on-failure
    ##    ports:
    ##      - "10075:3005"
    ##    environment:
    ##      PORT: "3005"
    ##      BITCOIN_HOST: "bitcoind"
    ##      RPC_PORT: $BITCOIN_RPC_PORT
    ##      RPC_USER: $BITCOIN_RPC_USER
    ##      RPC_PASSWORD: $BITCOIN_RPC_PASS
    ##      BITCOIN_RPC_HIDDEN_SERVICE: "/var/lib/tor/bitcoin-rpc/hostname"
    ##      BITCOIN_P2P_HIDDEN_SERVICE: "/var/lib/tor/bitcoin-p2p/hostname"
    #
    #    # ----------------------------------------
    #    # Backups and Volume Snapshotting
    #    # ----------------------------------------
    #    # docker-compose and volume management is
    #    # kinda pain in the ass comparing it to local
    #    # aka block volume management, I'm sure there
    #    # are tools that avoid such struggle with volumes
    #    # such as automatic backup to some S3 and easy
    #    # recovery.
    #    # ----------------------------------------

    # TODO: Check Docker Volume Plugins as they may be useful for managing these volumes, also after quick
    #   research I think they would have me in my previous production deployments based on docker-compose.include:
    #   https://github.com/rexray/rexray
    #   https://github.com/rancher/convoy
    #   https://github.com/MatchbookLab/local-persist
    #   https://github.com/ScatterHQ/flocker
    # TODO(https://github.com/keinsell/plygrnd/issues/335): ZFS and Docker Volumes?

volumes:
  redis_data:
    name: redis
    driver: local
  minio_data:
    name: minio
    driver: local
  twenty_data:
    name: twenty
    driver: local
  cockroach_data:
    name: cockroach
    driver: local
  prometheus_data:
    name: prometheus
    driver: local
  grafana_data:
    name: grafana
    driver: local
  bytebase_data:
    name: bytebase
    driver: local
  pg_config:
    name: pgconfig
    driver: local
  postgres_data:
    name: postgres_data
    driver: local
  supabase_storage_data:
    driver: local
    name: supabase-storage
  bitcoin_data:
    driver: local
    name: bitcoin
  hydra_data:
    driver: local
    name: hydra

networks:
  plygrnd:
    name: plygrnd
  infisical:
    name: infisical
  cache:
    name: cache
  sourcegraph:
    name: sourcegraph
  supabase:
    name: supabase
  twentycrm:
    name: twentycrm
  cockroach:
    name: cockroach
  monitoring:
    name: monitoring
    driver: bridge
  postgres:
    name: postgres
  tunnel:
    name: tunnel
    external: true
  bitcoin:
    name: bitcoin
  docker_proxy:
    name: docker_proxy
  reverse_proxy:
    name: reverse_proxy
    external: true

# TODO: Learn how to utilize secrets.
secrets:
  postgres_password:
    environment: "POSTGRES_PASSWORD"
    name: postgres_password

# TODO: Learn how to utilize configs.
configs:
  postgres_config:
    name: postgres-config
    file: .env

